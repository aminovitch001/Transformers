# Simplified Vision Transformer (ViT)

## üìå Project Overview
The goal of this practical work is to implement a **simplified version of the Vision Transformer (ViT)** model, inspired by the paper:

> *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*  
> Alexey Dosovitskiy et al., 2021  

We will create a **basic version**, much smaller than the original models in the paper, and without relying on complex data augmentation or regularization techniques.  
It is important to note that such techniques are essential for transformers to perform effectively on large-scale datasets.

After manually implementing the ViT, we will also explore the [`timm`](https://github.com/huggingface/pytorch-image-models) library, which provides pre-built implementations of many popular vision architectures.

## üöÄ Objectives
- Understand the core components of the Vision Transformer architecture.
- Implement a minimal ViT model from scratch.
- Train and test the model on a small dataset.
- Compare results with `timm` pre-trained models.

## üõ†Ô∏è Technologies
- Python
- PyTorch
- timm (for pre-built models)


